{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Dask on AzureML\n",
    "\n",
    "This notebook shows how to run a Dask cluster on an AzureML Compute cluster. \n",
    "For setup instructions of you python environment, please see the [Readme](../README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "# from azureml.train.estimator import Estimator\n",
    "from azureml.widgets import RunDetails\n",
    "# from azureml.core.runconfig import MpiConfiguration\n",
    "# from azureml.core import VERSION\n",
    "import uuid\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create AML Compute Cluster \n",
    "\n",
    "As you need to connect to compute cluster via Compute Instance, make sure you create __SSH enabled__ AML Compute Cluster. And of course remember the ID and Password forport-forwarding from Notebook VM to Dask Scheduler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"dask-inter-cpu-1\"\n",
    "username=''\n",
    "adminusersshkey = ''\n",
    "MAXNODE=4\n",
    "\n",
    "if username != '' or adminusersshkey !='':\n",
    "    # Verify that cluster does not exist already\n",
    "    try:\n",
    "        dask_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "        print('Found existing cluster, use it.')\n",
    "    except ComputeTargetException:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS3_V2',\n",
    "                                                            min_nodes=0, max_nodes=MAXNODE,\n",
    "                                                            remote_login_port_public_access='Enabled',\n",
    "                                                            admin_username=username, \n",
    "                                                            admin_user_ssh_key=adminusersshkey)\n",
    "        dask_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "    dask_cluster.wait_for_completion(show_output=True)\n",
    "else:\n",
    "    print('Check your user name and password')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'dask-DS2-V22' was created from AML Stduio\n",
    "# SSH is Enabled\n",
    "dask_cluster = ws.compute_targets[cpu_cluster_name]\n",
    "dask_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting the Dask cluster using an Estimator with MpiConfiguration. Make sure the cluster is able to scale up to few nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "daskEnv = Environment.from_conda_specification('interactieDask', './conda.yml')\n",
    "daskEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = ws.get_default_datastore()\n",
    "data_ref = datastore.path('./').as_mount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.runconfig import PyTorchConfiguration\n",
    "\n",
    "MAXNODE=4\n",
    "src = ScriptRunConfig(source_directory='./source',\n",
    "                       script='startDask.py',\n",
    "                       arguments=['--output_folder', str(data_ref)],\n",
    "                       environment=daskEnv,\n",
    "                       compute_target=dask_cluster,\n",
    "                       distributed_job_config=PyTorchConfiguration(node_count=MAXNODE)\n",
    "                       )\n",
    "# src.run_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = Experiment(ws, 'startDask').submit(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
    "experiment_name = 'dask-interactive-cluster'\n",
    "mlflow.set_experiment(experiment_name)\n",
    "mlflowrun = mlflow.get_run(run.id)\n",
    "\n",
    "\n",
    "# some helper to generate URLs later\n",
    "class DaskURLs:\n",
    "    def __init__(self, bokeh_port, jupyter_port, jupyter_token):\n",
    "        self.bokeh_port = bokeh_port\n",
    "        self.jupyter_port = jupyter_port\n",
    "        self.jupyter_token = jupyter_token\n",
    "        \n",
    "    def _repr_javascript_(self):\n",
    "        return f'''\n",
    "        var hostname = window.location.hostname\n",
    "        var dot = hostname.indexOf('.')\n",
    "        var first = hostname.substr(0, dot)\n",
    "        var last = hostname.substr(dot)\n",
    "        var bokeh = 'https://' + first +'-{self.bokeh_port}'+ last\n",
    "        var jupyter = 'https://' + first +'-{self.jupyter_port}'+ last+'?token={self.jupyter_token}'\n",
    "        element.html(`\n",
    "            Bokeh: <a href=`+bokeh+` target='bokeh'>`+bokeh+`</a><br>\n",
    "            Jupyter: <a href=`+jupyter+` target='jupyter'>`+jupyter+`</a><br>`)\n",
    "        '''\n",
    "\n",
    "print(\"waiting for scheduler node's ip\")\n",
    "while not '' in mlflowrun._data.params['headnode']:\n",
    "    print('.', end =\"\")\n",
    "    time.sleep(5)\n",
    "\n",
    "clear_output()\n",
    "headnode_private_ip = mlflowrun._data.params['headnode']\n",
    "print('Headnode has IP:', headnode_private_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's find the public IP and ssh port of the head node\n",
    "\n",
    "headnode_public_ip = None\n",
    "headnode_ssh_port = None\n",
    "for node in dask_cluster.list_nodes():\n",
    "    if node['privateIpAddress'] == headnode_private_ip:\n",
    "        headnode_public_ip = node['publicIpAddress']\n",
    "        headnode_ssh_port = node['port']\n",
    "        break\n",
    "        \n",
    "if headnode_public_ip == None:\n",
    "    print('Headnode not found in cluster')\n",
    "else:\n",
    "    print(f'Headnode is at {headnode_public_ip}:{headnode_ssh_port}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish the port-forwarding from Notebook VM to Dask Scheduler\n",
    "Since Notebook VM does not yet support VNets, you need to build an SSH port forwarder through SSH login.\n",
    "\n",
    "In the prior cell we looked up the public IP and port of the headnode of the cluster \n",
    "\n",
    "Now, open the terminal on the Notebook VM and type what the following cell outputs  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ssh azureuser@{headnode_public_ip} -p {headnode_ssh_port} -L 8786:localhost:8786 -L 8788:{headnode_private_ip}:8787 -L 9999:localhost:8888')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to leave the terminal tab open to keep the port-forward running\n",
    "\n",
    "As you see, you are forwarding 3 ports \n",
    "\n",
    "1. 8786 is for the scheduler and will be used to connect the client to the cluster\n",
    "2. 8788 is for the Bokeh app that shows the activity on the cluster (we are mapping to the local port 8788 to avoid a conflict with the RStudio Server running on the Notebook VM)\n",
    "3. 9999 is for a jupyter instance running on the head node. You can connect to the scheduler from the jupyter running on your Notebook VM or from this jupyter instance on the head node.   \n",
    "\n",
    "To access the Bokeh app, change the URL to your notebook VM by adding `-8788` right after the machine name. If you are running this notebook on a Notebook VM, then you can create the URLs by excuting the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"waiting for jupyter token\")\n",
    "while not '' in mlflowrun._data.params['jupyter-token']:\n",
    "    print('.', end =\"\")\n",
    "    time.sleep(5)\n",
    "\n",
    "# this will only work when running on a Notebook VM\n",
    "DaskURLs('8788', '9999', mlflowrun._data.params['jupyter-token'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you are seeing this after you clicked on the Bokeh link and then select 'Status':\n",
    "\n",
    "![Bokeh](../img/bokeh.png)\n",
    "\n",
    "If you are wondering what all this port business in accomplishing, please see the graph below that tries to illustrate who talks to whom and how.\n",
    "\n",
    "![Network](../img/network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run some jobs on the cluster\n",
    "If you are able to see the Bokeh app, it is time to use the cluster. Thanks to the port forward, the scheduler appears to the notebook VM at `tcp://localhost:8786`. You should see 10 workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "c = Client('tcp://localhost:8786')\n",
    "c.restart()\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if the cluster works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from dask import delayed, visualize\n",
    "\n",
    "def inc(x):\n",
    "    time.sleep(abs(np.random.normal(5, 2)))\n",
    "    return x + 1\n",
    "\n",
    "fut = []\n",
    "for i in range(10):\n",
    "    fut.append( c.submit(delayed(inc), i) )\n",
    "\n",
    "fut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in fut:\n",
    "    print(i.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum(a):\n",
    "    x = 0\n",
    "    for y in a:\n",
    "        x += y\n",
    "    return x\n",
    "\n",
    "results = []\n",
    "for f in fut:\n",
    "    results.append(f.result())\n",
    "    \n",
    "fut2 = c.submit(sum, results)\n",
    "fut2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fut2.result().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use terminal to install graphviz \n",
    "```bash\n",
    "sudo apt-get install graphviz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(fut2.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on Large Datasets\n",
    "(from https://github.com/dask/dask-tutorial)\n",
    "\n",
    "Sometimes you'll want to train on a larger than memory dataset. `dask-ml` has implemented estimators that work well on dask arrays and dataframes that may be larger than your machine's RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import joblib\n",
    "import dask.array as da\n",
    "import dask.delayed\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a small (random) dataset locally using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_centers = 12\n",
    "n_features = 20\n",
    "\n",
    "X_small, y_small = make_blobs(n_samples=1000, centers=n_centers, n_features=n_features, random_state=0)\n",
    "\n",
    "centers = np.zeros((n_centers, n_features))\n",
    "\n",
    "for i in range(n_centers):\n",
    "    centers[i] = X_small[y_small == i].mean(0)\n",
    "    \n",
    "centers[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The small dataset will be the template for our large random dataset.\n",
    "We'll use `dask.delayed` to adapt `sklearn.datasets.make_blobs`, so that the actual dataset is being generated on our workers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_per_block = 100000\n",
    "n_blocks = 500\n",
    "\n",
    "delayeds = [dask.delayed(make_blobs)(n_samples=n_samples_per_block,\n",
    "                                     centers=centers,\n",
    "                                     n_features=n_features,\n",
    "                                     random_state=i)[0]\n",
    "            for i in range(n_blocks)]\n",
    "arrays = [da.from_delayed(obj, shape=(n_samples_per_block, n_features), dtype='float64')\n",
    "          for obj in delayeds]\n",
    "X = da.concatenate(arrays)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the size of the array GB\n",
    "X.nbytes / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this on the cluster.\n",
    "X = X.persist()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithms implemented in Dask-ML are scalable. They handle larger-than-memory datasets just fine.\n",
    "\n",
    "They follow the scikit-learn API, so if you're familiar with scikit-learn, you'll feel at home with Dask-ML.\n",
    "\n",
    "Install Dask-ML before you run following codes\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge dask-ml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.cluster import KMeans\n",
    "clf = KMeans(init_max_iter=3, oversampling_factor=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time clf.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.labels_[:10].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shut cluster down\n",
    "To shut the cluster down, cancel the job that runs the cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in ws.experiments['startDask'].get_runs():\n",
    "    if run.get_status() == \"Running\":\n",
    "        print(f'cancelling run {run.id}')\n",
    "        run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just for convenience, get the latest running Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in ws.experiments['startDask'].get_runs():\n",
    "    if run.get_status() == \"Running\":\n",
    "        print(f'latest running run is {run.id}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6865bf3d0c8cd76df395d5f2226878eaf83cb8580fb53dcdaa2bb9522d89ec7"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
