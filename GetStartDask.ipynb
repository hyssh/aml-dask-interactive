{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Dask on AzureML\n",
    "\n",
    "This notebook shows how to run a Dask cluster on an AzureML Compute cluster. \n",
    "For setup instructions of you python environment, please see the [Readme](./README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from azureml.core import Workspace, Experiment, Environment, Datastore, Dataset, ScriptRunConfig\n",
    "from azureml.core.runconfig import PyTorchConfiguration\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access to Azure Machine Learning Service\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create AML Compute Cluster \n",
    "\n",
    "As you need to connect to compute cluster via Compute Instance, make sure you create __SSH enabled__ AML Compute Cluster. And of course remember the ID and Password forport-forwarding from your local host (PC/MAC) to Dask Scheduler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"dask-inter-cpu-1\"\n",
    "VMSIZE='STANDARD_DS3_V2'\n",
    "USERNAME=''\n",
    "ADMINUSERSSHKEY = ''\n",
    "MAXNODE=2\n",
    "VNETRGNAME=''\n",
    "VNETNAME=''\n",
    "SUBNETNAME=''\n",
    "\n",
    "\n",
    "if username != '' or adminusersshkey !='':\n",
    "    # Verify that cluster does not exist already\n",
    "    try:\n",
    "        dask_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "        print('Found existing cluster, use it.')\n",
    "    except ComputeTargetException:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size=VMSIZE,\n",
    "                                                            min_nodes=0, max_nodes=MAXNODE,\n",
    "                                                            remote_login_port_public_access='Enabled',\n",
    "                                                            admin_username=USERNAME, \n",
    "                                                            admin_user_ssh_key=ADMINUSERSSHKEY,\n",
    "                                                            vnet_resourcegroup_name=VNETRGNAME,\n",
    "                                                            vnet_name=VNETNAME,\n",
    "                                                            subnet_name=SUBNETNAME)\n",
    "        dask_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "    dask_cluster.wait_for_completion(show_output=True)\n",
    "else:\n",
    "    print('Check your user name and password')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "daskEnv = Environment.from_conda_specification('interactieDask', './conda.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register sample storage account for this \n",
    "\n",
    "If you want to use your contain in a storage account, please update following account name and sas_token accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_name = 'mtcseattle'\n",
    "container_name = 'azure-service-classifier'\n",
    "account_name = 'mtcseattle'\n",
    "sas_token = '?sv=2020-04-08&st=2021-05-26T04%3A39%3A46Z&se=2022-05-27T04%3A39%3A00Z&sr=c&sp=rl&sig=CTFMEu24bo2X06G%2B%2F2aKiiPZBzvlWHELe15rNFqULUk%3D'\n",
    "\n",
    "datastore = Datastore.register_azure_blob_container(workspace=ws, \n",
    "                                                    datastore_name=datastore_name, \n",
    "                                                    container_name=container_name,\n",
    "                                                    account_name=account_name, \n",
    "                                                    sas_token=sas_token,\n",
    "                                                    overwrite=True)\n",
    "datastore = Datastore.get(ws, 'mtcseattle')\n",
    "\n",
    "inputDataset = Dataset.File.from_files(path=(datastore, 'data'))\n",
    "\n",
    "# This is optional \n",
    "# inputDataset = inputDataset.register(workspace=ws,\n",
    "#                                        name='Azure Services Dataset',\n",
    "#                                        description='Dataset containing azure related posts on Stackoverflow',\n",
    "#                                        create_new_version=True)\n",
    "\n",
    "# inputDataset = Dataset.get_by_name(ws, 'Azure Services Dataset')\n",
    "# inputDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set ScriptRunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src = ScriptRunConfig(source_directory='./source',\n",
    "                       script='startDask.py',\n",
    "                       arguments=[inputDataset.as_mount(path_on_compute='data')],\n",
    "                       environment=daskEnv,\n",
    "                       compute_target=dask_cluster,\n",
    "                       distributed_job_config=PyTorchConfiguration(node_count=MAXNODE)\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the ScriptRunConfig in a Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expName = 'Interactive Dask Cluster'\n",
    "run = Experiment(ws, expName).submit(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get IP address of headnode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
    "mlflow.set_experiment(expName)\n",
    "mlflowrun = mlflow.get_run(run.id)\n",
    "\n",
    "print(\"waiting for scheduler node's ip\")\n",
    "while not 'headnode' in mlflowrun._data.params:\n",
    "    print('.', end =\"\")\n",
    "    time.sleep(5)\n",
    "\n",
    "clear_output()\n",
    "headnode_private_ip = mlflowrun._data.params['headnode']\n",
    "print('Headnode has IP:', headnode_private_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's find the public IP and ssh port of the head node\n",
    "headnode_public_ip = None\n",
    "headnode_ssh_port = None\n",
    "for node in dask_cluster.list_nodes():\n",
    "    if node['privateIpAddress'] == headnode_private_ip:\n",
    "        headnode_public_ip = node['publicIpAddress']\n",
    "        headnode_ssh_port = node['port']\n",
    "        break\n",
    "        \n",
    "if headnode_public_ip == None:\n",
    "    print('Headnode not found in cluster')\n",
    "else:\n",
    "    print(f'Headnode is at {headnode_public_ip}:{headnode_ssh_port}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish the port-forwarding from your localhost to Dask Scheduler\n",
    "Since Notebook VM does not yet support VNets, you need to build an SSH port forwarder through SSH login.\n",
    "\n",
    "In the prior cell we looked up the public IP and port of the headnode of the cluster \n",
    "\n",
    "Now, open the terminal on the Notebook VM and type what the following cell outputs  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ssh {USERNAME}@{headnode_public_ip} -p {headnode_ssh_port} -L 8786:localhost:8786 -L 8788:{headnode_private_ip}:8787 -L 9999:localhost:8888')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to leave the terminal tab open to keep the port-forward running\n",
    "\n",
    "![ssh](./img/port-forwarding.png)\n",
    "\n",
    "As you see, you are forwarding 3 ports \n",
    "\n",
    "1. 8786 is for the scheduler and will be used to connect the client to the cluster\n",
    "2. 8788 is for the Bokeh app that shows the activity on the cluster (we are mapping to the local port 8788 to avoid a conflict with the RStudio Server running on the Notebook VM)\n",
    "3. 9999 is for a jupyter instance running on the head node. You can connect to the scheduler from the jupyter running on your Notebook VM or from this jupyter instance on the head node.   \n",
    "\n",
    "To access the Bokeh app, change the URL to your notebook VM by adding `-8788` right after the machine name. If you are running this notebook on a Notebook VM, then you can create the URLs by excuting the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"waiting for jupyter token\")\n",
    "while not 'jupyter-token' in mlflowrun._data.params:\n",
    "    print('.', end =\"\")\n",
    "    time.sleep(5)\n",
    "\n",
    "clear_output()\n",
    "jupyterToken = mlflowrun._data.params['jupyter-token']\n",
    "print('Notebook url:')\n",
    "print(f'http://localhost:9999/notebooks?token={jupyterToken}')\n",
    "print('Bokeh url:')\n",
    "print(f'http://localhost:8788')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you are seeing this after you clicked on the Bokeh link and then select 'Status':\n",
    "\n",
    "![Bokeh](./img/bokeh.png)\n",
    "\n",
    "If you are wondering what all this port business in accomplishing, please see the graph below that tries to illustrate who talks to whom and how.\n",
    "\n",
    "![Network](./img/network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run some jobs on the cluster\n",
    "If you are able to see the Bokeh app, it is time to use the cluster. Thanks to the port forward, the scheduler appears to the notebook VM at `tcp://localhost:8786`. You should see 10 workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shut cluster down\n",
    "To shut the cluster down, cancel the job that runs the cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in ws.experiments[expName].get_runs():\n",
    "    if run.get_status() == \"Running\":\n",
    "        print(f'cancelling run {run.id}')\n",
    "        run.cancel()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6865bf3d0c8cd76df395d5f2226878eaf83cb8580fb53dcdaa2bb9522d89ec7"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
